{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8dadaa2",
   "metadata": {},
   "source": [
    "# ML Model Building - Submission\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading preprocessed data with NLP embeddings\n",
    "2. Feature selection to create a reduced feature set\n",
    "3. Building two models:\n",
    "   - **Model 1 (M1):** Full feature set\n",
    "   - **Model 2 (M2):** Reduced feature set (selected features only)\n",
    "4. Saving both models as `.sav` files for future use\n",
    "\n",
    "**Note:** This submission focuses on model building and saving. Performance evaluation will be included in the next phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d8b50",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1776500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n",
      "âœ“ Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"âœ“ Random seed set to: 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f34ca",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9c5b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (89302, 109)\n",
      "Total samples: 89,302\n",
      "Total features (including target): 109\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>DLC count</th>\n",
       "      <th>About the game</th>\n",
       "      <th>Windows</th>\n",
       "      <th>Mac</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Achievements</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>Release_Month</th>\n",
       "      <th>Release_Day</th>\n",
       "      <th>...</th>\n",
       "      <th>Publishers_LTD</th>\n",
       "      <th>Publishers_EroticGamesClub</th>\n",
       "      <th>Publishers_Square_Enix</th>\n",
       "      <th>Publishers_Strategy_First</th>\n",
       "      <th>Publishers_HH_Games</th>\n",
       "      <th>Publishers_Choice_of_Games</th>\n",
       "      <th>Publishers_Sekai_Project</th>\n",
       "      <th>Publishers_Electronic_Arts</th>\n",
       "      <th>Publishers_Atomic_Fabrik</th>\n",
       "      <th>popularity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.007898</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[ 2.43655536e-02 -4.33482192e-02 -1.89679326e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.057969</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.963858</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[-1.18375435e-01  6.85120896e-02 -8.45908746e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.049630</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.338225</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[-7.47546032e-02 -1.29440166e-02  2.83202082e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.121362</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.181817</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[ 2.98691001e-02  1.47273587e-02  5.98186301e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.121362</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.118702</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[-6.26481697e-02  7.47049646e-03 -5.16111143e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price  DLC count                                     About the game  \\\n",
       "0  2.007898  -0.039159  [ 2.43655536e-02 -4.33482192e-02 -1.89679326e-...   \n",
       "1 -0.963858  -0.039159  [-1.18375435e-01  6.85120896e-02 -8.45908746e-...   \n",
       "2 -0.338225  -0.039159  [-7.47546032e-02 -1.29440166e-02  2.83202082e-...   \n",
       "3 -0.181817  -0.039159  [ 2.98691001e-02  1.47273587e-02  5.98186301e-...   \n",
       "4 -1.118702  -0.039159  [-6.26481697e-02  7.47049646e-03 -5.16111143e-...   \n",
       "\n",
       "   Windows    Mac  Linux  Achievements  Release_Year  Release_Month  \\\n",
       "0     True  False  False      0.057969          2008             10   \n",
       "1     True   True  False     -0.049630          2017             10   \n",
       "2     True  False  False     -0.121362          2021             11   \n",
       "3     True   True   True     -0.121362          2020              7   \n",
       "4     True   True  False     -0.019741          2020              2   \n",
       "\n",
       "   Release_Day  ...  Publishers_LTD  Publishers_EroticGamesClub  \\\n",
       "0           21  ...               0                           0   \n",
       "1           12  ...               0                           0   \n",
       "2           17  ...               0                           0   \n",
       "3           23  ...               0                           0   \n",
       "4            3  ...               0                           0   \n",
       "\n",
       "   Publishers_Square_Enix  Publishers_Strategy_First  Publishers_HH_Games  \\\n",
       "0                       0                          0                    0   \n",
       "1                       0                          0                    0   \n",
       "2                       0                          0                    0   \n",
       "3                       0                          0                    0   \n",
       "4                       0                          0                    0   \n",
       "\n",
       "   Publishers_Choice_of_Games  Publishers_Sekai_Project  \\\n",
       "0                           0                         0   \n",
       "1                           0                         0   \n",
       "2                           0                         0   \n",
       "3                           0                         0   \n",
       "4                           0                         0   \n",
       "\n",
       "   Publishers_Electronic_Arts  Publishers_Atomic_Fabrik  popularity_class  \n",
       "0                           0                         0               Low  \n",
       "1                           0                         0               Low  \n",
       "2                           0                         0               Low  \n",
       "3                           0                         0               Low  \n",
       "4                           0                         0               Low  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset with NLP embeddings\n",
    "df = pd.read_csv('data/processed/games_preprocessed.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total samples: {df.shape[0]:,}\")\n",
    "print(f\"Total features (including target): {df.shape[1]}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3c91d",
   "metadata": {},
   "source": [
    "# Convert Embedding Strings to Numeric Arrays\n",
    "\n",
    "The 'About the game' column contains embedding vectors stored as strings. We need to convert them back to numeric arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5448465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'About the game' embeddings from strings to numeric arrays...\n",
      "âœ“ Conversion complete!\n",
      "âœ“ Sample embedding shape: (384,)\n",
      "âœ“ Sample embedding (first 10 values): [ 0.02436555 -0.04334822 -0.00189679 -0.03764986 -0.08963642  0.02961544\n",
      " -0.0579943   0.0187653   0.01877719  0.06303879]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Check if 'About the game' column exists and contains string representations of arrays\n",
    "if 'About the game' in df.columns:\n",
    "    print(\"Converting 'About the game' embeddings from strings to numeric arrays...\")\n",
    "    \n",
    "    # Convert string representations to actual arrays\n",
    "    def string_to_array(s):\n",
    "        if isinstance(s, str):\n",
    "            # Remove extra whitespace and convert to numpy array\n",
    "            return np.fromstring(s.strip('[]'), sep=' ')\n",
    "        return s\n",
    "    \n",
    "    df['About the game'] = df['About the game'].apply(string_to_array)\n",
    "    \n",
    "    print(f\"âœ“ Conversion complete!\")\n",
    "    print(f\"âœ“ Sample embedding shape: {df['About the game'].iloc[0].shape}\")\n",
    "    print(f\"âœ“ Sample embedding (first 10 values): {df['About the game'].iloc[0][:10]}\")\n",
    "else:\n",
    "    print(\"'About the game' column not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acd0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding embeddings into separate columns...\n",
      "âœ“ Expanded embeddings into 384 numeric columns\n",
      "âœ“ New dataset shape: (89302, 492)\n",
      "âœ“ First few column names: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4']\n",
      "âœ“ Last few column names: ['Publishers_Choice_of_Games', 'Publishers_Sekai_Project', 'Publishers_Electronic_Arts', 'Publishers_Atomic_Fabrik', 'popularity_class']\n"
     ]
    }
   ],
   "source": [
    "# Expand embeddings into separate columns\n",
    "# This is necessary because sklearn models need 2D numeric arrays, not arrays within cells\n",
    "\n",
    "if 'About the game' in df.columns:\n",
    "    print(\"Expanding embeddings into separate columns...\")\n",
    "    \n",
    "    # Convert the 'About the game' column (which contains arrays) into separate columns\n",
    "    embeddings_list = df['About the game'].tolist()\n",
    "    embeddings_df = pd.DataFrame(embeddings_list, \n",
    "                                  columns=[f'embedding_{i}' for i in range(len(embeddings_list[0]))])\n",
    "    \n",
    "    # Drop the original 'About the game' column\n",
    "    df = df.drop('About the game', axis=1)\n",
    "    \n",
    "    # Insert embedding columns at the beginning\n",
    "    df = pd.concat([embeddings_df, df], axis=1)\n",
    "    \n",
    "    print(f\"âœ“ Expanded embeddings into {len(embeddings_list[0])} numeric columns\")\n",
    "    print(f\"âœ“ New dataset shape: {df.shape}\")\n",
    "    print(f\"âœ“ First few column names: {df.columns[:5].tolist()}\")\n",
    "    print(f\"âœ“ Last few column names: {df.columns[-5:].tolist()}\")\n",
    "else:\n",
    "    print(\"No embedding expansion needed - 'About the game' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7fb877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution:\n",
      "popularity_class\n",
      "Low       78429\n",
      "Medium     8934\n",
      "High       1939\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "popularity_class\n",
      "Low       87.824461\n",
      "Medium    10.004255\n",
      "High       2.171284\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target variable distribution:\")\n",
    "print(df['popularity_class'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['popularity_class'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc93a1",
   "metadata": {},
   "source": [
    "# Prepare Data for Modeling\n",
    "\n",
    "Separate features (X) and target variable (y), then split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6c90a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (89302, 491)\n",
      "Target (y) shape: (89302,)\n",
      "\n",
      "Total number of features: 491\n",
      "\n",
      "Feature data types:\n",
      "float64    387\n",
      "int64      101\n",
      "bool         3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('popularity_class', axis=1)\n",
    "y = df['popularity_class']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nTotal number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature data types:\")\n",
    "print(X.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b53f6",
   "metadata": {},
   "source": [
    "# Handle Non-Numeric Columns\n",
    "\n",
    "Check for and handle any remaining categorical/text columns that need to be encoded or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc56ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for non-numeric columns...\n",
      "\n",
      "Data types in features:\n",
      "float64    387\n",
      "int64      101\n",
      "bool         3\n",
      "Name: count, dtype: int64\n",
      "âœ“ All columns are numeric!\n",
      "\n",
      "Final feature set shape: (89302, 491)\n",
      "Final feature types:\n",
      "float64    387\n",
      "int64      101\n",
      "bool         3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns in X\n",
    "print(\"Checking for non-numeric columns...\")\n",
    "print(f\"\\nData types in features:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "# Identify object (string) columns\n",
    "object_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if object_cols:\n",
    "    print(f\"\\nâš  Found {len(object_cols)} non-numeric columns:\")\n",
    "    for col in object_cols:\n",
    "        print(f\"  - {col}: {X[col].nunique()} unique values\")\n",
    "        print(f\"    Sample values: {X[col].dropna().head(3).tolist()}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Dropping non-numeric columns: {object_cols}\")\n",
    "    X = X.drop(columns=object_cols)\n",
    "    print(f\"âœ“ Remaining features: {X.shape[1]}\")\n",
    "else:\n",
    "    print(\"âœ“ All columns are numeric!\")\n",
    "\n",
    "print(f\"\\nFinal feature set shape: {X.shape}\")\n",
    "print(f\"Final feature types:\\n{X.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81985d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 71441 samples\n",
      "Testing set size: 17861 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets (80/20 split)\n",
    "# Use stratify to maintain class distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbec3a",
   "metadata": {},
   "source": [
    "# Model 1: Random Forest with Full Feature Set\n",
    "\n",
    "Train a Random Forest Classifier using all available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b5a1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1 with full feature set...\n",
      "âœ“ Model 1 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier for Model 1 (Full Feature Set)\n",
    "model_M1 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Model 1 with full feature set...\")\n",
    "# Fit the model on training set\n",
    "model_M1.fit(X_train, y_train)\n",
    "print(\"âœ“ Model 1 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb11fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model 1 saved as: finalized_model_M1.sav\n",
      "âœ“ Model 1 loaded and verified\n",
      "âœ“ Test score: 0.8839370695929679\n",
      "âœ“ F1score: 0.8377274845027388\n"
     ]
    }
   ],
   "source": [
    "# Save Model 1 to disk\n",
    "import sklearn\n",
    "filename_M1 = 'finalized_model_M1.sav'\n",
    "pickle.dump(model_M1, open(filename_M1, 'wb'))\n",
    "print(f\"âœ“ Model 1 saved as: {filename_M1}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_M1 = pickle.load(open(filename_M1, 'rb'))\n",
    "result_M1 = loaded_model_M1.score(X_test, y_test)\n",
    "print(f\"âœ“ Model 1 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_M1}\")\n",
    "\n",
    "# Use f1 score as well since accuracy may be insufficient for imbalanced classes\n",
    "# f1 = 2 * [(precision*recall)/(precision+recall)]\n",
    "# this way we can account for precision and recall as well as accuracy\n",
    "y_pred = loaded_model_M1.predict(X_test)\n",
    "modelF1Score = sklearn.metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"âœ“ F1score: {modelF1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d48d35",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Use SelectKBest with ANOVA F-statistic to select the most important features for Model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6e5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before SelectKBest with ANOVA:  (89302, 491)\n",
      "Features after SelectKBest with ANOVA:   (89302, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features before SelectKBest with ANOVA: \", X.shape)\n",
    "X_new = SelectKBest(f_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "print(\"Features after SelectKBest with ANOVA:  \", X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a683b",
   "metadata": {},
   "source": [
    "# Model 2: Random Forest with Reduced Feature Set\n",
    "\n",
    "Re-split data with X_new obtained from SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e8c9ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 71441 samples\n",
      "Testing set size: 17861 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets (80/20 split)\n",
    "# Use stratify to maintain class distribution in both sets\n",
    "X_NewTrain, X_NewTest, y_NewTrain, y_NewTest = train_test_split(\n",
    "    X_new, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3a681",
   "metadata": {},
   "source": [
    "Train a Random Forest Classifier using only the selected (20 best) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740855e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 2 with full feature set...\n",
      "âœ“ Model 2 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier for Model 2 (Best Features)\n",
    "model_M2 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Model 2 with full feature set...\")\n",
    "# Fit the model on training set\n",
    "model_M2.fit(X_NewTrain, y_NewTrain)\n",
    "print(\"âœ“ Model 2 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27a7a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model 1 saved as: finalized_model_M2.sav\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 491 features, but RandomForestClassifier is expecting 20 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load the model from disk to verify\u001b[39;00m\n\u001b[32m      8\u001b[39m loaded_model_M2 = pickle.load(\u001b[38;5;28mopen\u001b[39m(filename_M2, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m result_M2 = \u001b[43mloaded_model_M2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Model 2 loaded and verified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Test score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_M1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:548\u001b[39m, in \u001b[36mClassifierMixin.score\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[33;03mReturn :ref:`accuracy <accuracy_score>` on provided data and labels.\u001b[39;00m\n\u001b[32m    525\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    544\u001b[39m \u001b[33;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight=sample_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:903\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    885\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    906\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:945\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    943\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    948\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 491 features, but RandomForestClassifier is expecting 20 features as input."
     ]
    }
   ],
   "source": [
    "# Save Model 2 to disk\n",
    "import sklearn\n",
    "filename_M2 = 'finalized_model_M2.sav'\n",
    "pickle.dump(model_M2, open(filename_M2, 'wb'))\n",
    "print(f\"âœ“ Model 2 saved as: {filename_M2}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_M2 = pickle.load(open(filename_M2, 'rb'))\n",
    "result_M2 = loaded_model_M2.score(X_NewTest, y_NewTest)\n",
    "print(f\"âœ“ Model 2 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_M2}\")\n",
    "\n",
    "# Use f1 score as well since accuracy may be insufficient for imbalanced classes\n",
    "# f1 = 2 * [(precision*recall)/(precision+recall)]\n",
    "# this way we can account for precision and recall as well as accuracy\n",
    "y_NewPred = loaded_model_M2.predict(X_NewTest)\n",
    "model2F1Score = sklearn.metrics.f1_score(y_NewTest, y_NewPred, average='weighted')\n",
    "print(f\"âœ“ F1score: {model2F1Score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
