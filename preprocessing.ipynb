{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ac6425",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8da7a",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "Loading the fixed games dataset. The original CSV had a header error where a comma was missing between Discount and DLC count, which was manually corrected and saved as games_fixed.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/raw/games_fixed.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total records: {df.shape[0]}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece172b9",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n",
    "Check basic information about the dataset including data types and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ec795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f27155",
   "metadata": {},
   "source": [
    "# Removing Irrelevant or \"Cheat\" Features\n",
    "Removing features that are:\n",
    "- Blatantly irrelevant (e.g., URLs, images, identifiers)\n",
    "- Would allow the model to \"cheat\" (e.g., Peak CCU, reviews, playtime stats measured after launch)\n",
    "\n",
    "Our goal is to predict game popularity **before or at launch**, so post-launch metrics must be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fbf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Features to drop\n",
    "dropped_cols = [\n",
    "    'AppID', 'Name', 'Required age', 'Discount', 'Peak CCU', 'Positive', 'Negative', 'Recommendations',\n",
    "    'Score rank', 'User score', 'Metacritic score', 'Metacritic url',\n",
    "    'Header image', 'Website', 'Support url', 'Support email',\n",
    "    'Reviews', 'Notes', 'Screenshots', 'Movies',\n",
    "    'Average playtime forever', 'Median playtime forever',\n",
    "    'Average playtime two weeks', 'Median playtime two weeks'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=dropped_cols)\n",
    "\n",
    "print(f\"\\nDropped {len(dropped_cols)} features\")\n",
    "print(f\"Remaining features: {df.shape[1]}\")\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ffb4a",
   "metadata": {},
   "source": [
    "# Feature Engineering - Release Date\n",
    "Parse the Release date field and extract year, month, and day as separate numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Release date' in df.columns:\n",
    "    print(\"Processing Release date field...\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df['Release date'] = pd.to_datetime(df['Release date'], errors='coerce')\n",
    "    \n",
    "    # Extract year, month, and day as separate features\n",
    "    df['Release_Year'] = df['Release date'].dt.year\n",
    "    df['Release_Month'] = df['Release date'].dt.month\n",
    "    df['Release_Day'] = df['Release date'].dt.day\n",
    "    \n",
    "    # Drop the original Release date column\n",
    "    df = df.drop(columns=['Release date'])\n",
    "    \n",
    "    print(\"Created new features: Release_Year, Release_Month, Release_Day\")\n",
    "    print(f\"Sample: Year={df['Release_Year'].iloc[0]}, Month={df['Release_Month'].iloc[0]}, Day={df['Release_Day'].iloc[0]}\")\n",
    "else:\n",
    "    print(\"Release date column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94045d4",
   "metadata": {},
   "source": [
    "# Target Variable Analysis and Transformation\n",
    "Analyzing 'Estimated owners' and creating a categorical target variable 'popularity_class' with three levels:\n",
    "- **Low**: ≤ 50,000 owners\n",
    "- **Medium**: 50,001 - 500,000 owners\n",
    "- **High**: > 500,000 owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in 'Estimated owners':\")\n",
    "print(df['Estimated owners'].value_counts().sort_index())\n",
    "\n",
    "# Check for and remove '0 - 0' range (noisy data)\n",
    "print(f\"\\nRows with '0 - 0' range: {(df['Estimated owners'] == '0 - 0').sum()}\")\n",
    "df = df[df['Estimated owners'] != '0 - 0'].copy()\n",
    "print(f\"Rows after removal: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_popularity(owners_range):\n",
    "    \"\"\"\n",
    "    Categorize game popularity based on estimated owners range.\n",
    "    Low: <= 50,000\n",
    "    Medium: 50,001 - 500,000\n",
    "    High: > 500,000\n",
    "    \"\"\"\n",
    "    if pd.isna(owners_range):\n",
    "        return None\n",
    "    \n",
    "    # Remove commas and parse the range\n",
    "    owners_range = owners_range.replace(',', '')\n",
    "    low, high = map(int, owners_range.split(' - '))\n",
    "    \n",
    "    if high <= 50000:\n",
    "        return 'Low'\n",
    "    elif high <= 500000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "# Apply categorization\n",
    "df['popularity_class'] = df['Estimated owners'].apply(categorize_popularity)\n",
    "\n",
    "# Check distribution\n",
    "print(\"Popularity class distribution:\")\n",
    "print(df['popularity_class'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['popularity_class'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original 'Estimated owners' column\n",
    "df = df.drop(columns=['Estimated owners'])\n",
    "\n",
    "# Move target variable to the end\n",
    "target = 'popularity_class'\n",
    "cols = [c for c in df.columns if c != target] + [target]\n",
    "df = df[cols]\n",
    "\n",
    "print(\"Dataset shape after target transformation:\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7a0ef",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "Identify features with missing values and determine appropriate imputation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02353322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing value analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Feature': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Data_Type': df.dtypes\n",
    "})\n",
    "\n",
    "missing_analysis = missing_analysis[missing_analysis['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "print(\"Features with missing values:\")\n",
    "print(missing_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy for handling missing values:\n",
    "# 1. Numeric features: Impute with median (robust to outliers)\n",
    "# 2. Categorical features: Impute with mode or 'Unknown'\n",
    "# 3. Features with >50% missing: Consider dropping\n",
    "\n",
    "# Separate numeric and categorical columns (excluding target)\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target from categorical\n",
    "if 'popularity_class' in categorical_cols:\n",
    "    categorical_cols.remove('popularity_class')\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(numeric_cols)\n",
    "print(f\"\\nCategorical columns: {len(categorical_cols)}\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# Impute numeric columns with median\n",
    "if numeric_cols:\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Impute categorical columns with most frequent value\n",
    "if categorical_cols:\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any remaining missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    rows_before = len(df)\n",
    "    df = df.dropna()\n",
    "    rows_after = len(df)\n",
    "    rows_dropped = rows_before - rows_after\n",
    "    \n",
    "    print(f\"\\nDropped {rows_dropped} rows with remaining missing values\")\n",
    "    print(f\"Rows before: {rows_before}\")\n",
    "    print(f\"Rows after: {rows_after}\")\n",
    "    print(f\"\\nFinal missing values: {df.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"\\nNo remaining missing values - all data has been successfully imputed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3813b6",
   "metadata": {},
   "source": [
    "# Handling Duplicates\n",
    "Check for and remove duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07200f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total records before duplicate removal: {len(df)}\")\n",
    "print(f\"Duplicate records: {df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f\"Total records after duplicate removal: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a57af7",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Numerical Features\n",
    "Analyze the distribution of numerical features and identify potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96320ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "print(\"Numerical Features Summary:\")\n",
    "print(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of key numerical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Select key numerical features to visualize\n",
    "key_numeric_features = numeric_cols[:6] if len(numeric_cols) >= 6 else numeric_cols\n",
    "\n",
    "for idx, col in enumerate(key_numeric_features):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].hist(df[col].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution of {col}')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "# Hide extra subplots if we have fewer than 6 features\n",
    "for idx in range(len(key_numeric_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e507f7",
   "metadata": {},
   "source": [
    "# Handling Outliers\n",
    "Detect and handle outliers using the IQR (Interquartile Range) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    Returns indices of outliers.\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers.index, lower_bound, upper_bound\n",
    "\n",
    "# Analyze outliers for each numeric column\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outlier_indices, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_count = len(outlier_indices)\n",
    "    outlier_pct = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'Outlier_Count': outlier_count,\n",
    "        'Outlier_Percentage': outlier_pct,\n",
    "        'Lower_Bound': lower,\n",
    "        'Upper_Bound': upper\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_Count', ascending=False)\n",
    "print(\"Outlier Analysis:\")\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787add9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap outliers using Winsorization to preserve data while reducing extreme values\n",
    "# This approach is preferred over removal to maintain sufficient training data\n",
    "print(f\"Dataset size before outlier handling: {len(df)}\")\n",
    "\n",
    "# Get updated list of numeric columns (now includes date features)\n",
    "current_numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Exclude date features and count variables from outlier handling\n",
    "date_features = ['Release_Year', 'Release_Month', 'Release_Day']\n",
    "count_features = ['DLC count', 'Achievements']  # Count variables with natural zero-inflation\n",
    "cols_to_cap = [col for col in current_numeric_cols if col not in date_features and col not in count_features]\n",
    "\n",
    "print(f\"Columns to cap for outliers: {len(cols_to_cap)}\")\n",
    "print(f\"Date features excluded from capping: {[col for col in date_features if col in df.columns]}\")\n",
    "print(f\"Count features excluded from capping: {[col for col in count_features if col in df.columns]}\")\n",
    "\n",
    "for col in cols_to_cap:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers\n",
    "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Ensure date features are integers (not floats)\n",
    "for col in date_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('Int64')  # Int64 handles NaN properly\n",
    "\n",
    "print(f\"\\nDataset size after outlier handling: {len(df)}\")\n",
    "print(\"Outliers have been capped to IQR bounds (excluding date and count features)\")\n",
    "print(\"Date features converted to integers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b67deb",
   "metadata": {},
   "source": [
    "# Feature Engineering - Categorical Features\n",
    "Analyze and encode categorical features. Check for high cardinality features that may need special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "categorical_info = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    top_value = df[col].mode()[0] if len(df[col].mode()) > 0 else None\n",
    "    top_value_count = (df[col] == top_value).sum() if top_value else 0\n",
    "    top_value_pct = (top_value_count / len(df)) * 100\n",
    "    \n",
    "    categorical_info.append({\n",
    "        'Feature': col,\n",
    "        'Unique_Values': unique_count,\n",
    "        'Top_Value': top_value,\n",
    "        'Top_Value_Percentage': top_value_pct\n",
    "    })\n",
    "\n",
    "categorical_info_df = pd.DataFrame(categorical_info).sort_values('Unique_Values', ascending=False)\n",
    "print(\"Categorical Features Analysis:\")\n",
    "print(categorical_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For high cardinality features (like Categories, Genres, Tags), \n",
    "# we'll use one-hot encoding or handle them separately\n",
    "# For now, let's check which features have manageable cardinality\n",
    "\n",
    "print(\"\\nFeatures by cardinality:\")\n",
    "print(\"Low cardinality (<10):\", [col for col in categorical_cols if df[col].nunique() < 10])\n",
    "print(\"Medium cardinality (10-50):\", [col for col in categorical_cols if 10 <= df[col].nunique() < 50])\n",
    "print(\"High cardinality (>=50):\", [col for col in categorical_cols if df[col].nunique() >= 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845cd1f",
   "metadata": {},
   "source": [
    "# Encoding Categorical Features\n",
    "For categorical features:\n",
    "- **Multi-value features** (Genres, Tags, Categories, Languages): Contain comma-separated values. Convert to binary features for the top 15 most common values.\n",
    "- **Single-value categorical features**: Remain as text for potential future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d592de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify multi-value features (Genres, Tags, Categories, Languages)\n",
    "# These features contain comma-separated values that need to be converted to binary features\n",
    "multi_value_feature_candidates = ['Genres', 'Tags', 'Categories', 'Supported languages', 'Full audio languages']\n",
    "multi_value_features = [col for col in multi_value_feature_candidates if col in df.columns]\n",
    "\n",
    "if multi_value_features:\n",
    "    print(\"Multi-value features found:\")\n",
    "    for col in multi_value_features:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].dropna().head(3))\n",
    "else:\n",
    "    print(\"No multi-value features found\")\n",
    "\n",
    "print(f\"\\nMulti-value features to process: {multi_value_features}\")\n",
    "\n",
    "# Separate single-value and multi-value categorical features\n",
    "single_value_categorical = [col for col in categorical_cols if col not in multi_value_features]\n",
    "print(f\"\\nSingle-value categorical features: {single_value_categorical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multi-value features (Categories, Genres, Tags), create binary features for top N values\n",
    "# This approach is commonly used for multi-label features\n",
    "\n",
    "import re\n",
    "\n",
    "def create_top_n_binary_features(df, column, top_n=20):\n",
    "    \"\"\"\n",
    "    Create binary features for the top N most common values in a multi-value column.\n",
    "    Uses exact matching to avoid substring false positives.\n",
    "    \"\"\"\n",
    "    # Get all values across all rows\n",
    "    all_values = []\n",
    "    for val in df[column].dropna():\n",
    "        if isinstance(val, str):\n",
    "            all_values.extend([v.strip() for v in val.split(',')])\n",
    "    \n",
    "    # Count occurrences\n",
    "    from collections import Counter\n",
    "    value_counts = Counter(all_values)\n",
    "    top_values = [val for val, count in value_counts.most_common(top_n)]\n",
    "    \n",
    "    print(f\"\\nTop {len(top_values)} values in {column}:\")\n",
    "    for val, count in value_counts.most_common(top_n):\n",
    "        print(f\"  {val}: {count}\")\n",
    "    \n",
    "    # Create binary features\n",
    "    for value in top_values:\n",
    "        # skip empty values\n",
    "        if not value or not value.strip():\n",
    "            continue\n",
    "        # make a safe feature name by replacing non-alphanumeric chars with underscores\n",
    "        safe_value = re.sub(r'[^0-9A-Za-z_]+', '_', value).strip('_')\n",
    "        # Skip if the safe_value is empty after cleaning\n",
    "        if not safe_value:\n",
    "            continue\n",
    "        feature_name = f\"{column}_{safe_value}\"\n",
    "        \n",
    "        # Use exact matching: check if value is in the comma-separated list\n",
    "        # This avoids substring matching issues (e.g., \"player\" matching \"Multiplayer\")\n",
    "        def check_exact_match(cell_value):\n",
    "            if pd.isna(cell_value) or not isinstance(cell_value, str):\n",
    "                return 0\n",
    "            # Split by comma and check if our value is in the list\n",
    "            values_list = [v.strip() for v in cell_value.split(',')]\n",
    "            return 1 if value in values_list else 0\n",
    "        \n",
    "        df[feature_name] = df[column].apply(check_exact_match)\n",
    "    \n",
    "    return df, top_values\n",
    "\n",
    "# Apply to multi-value features\n",
    "for col in multi_value_features:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing multi-value feature: {col}\")\n",
    "    df, top_values = create_top_n_binary_features(df, col, top_n=15)\n",
    "    print(f\"Created {len(top_values)} binary features for {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original multi-value categorical columns\n",
    "df = df.drop(columns=multi_value_features)\n",
    "print(f\"\\nDropped original multi-value columns: {multi_value_features}\")\n",
    "print(f\"Current dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c002e",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "Normalize numerical features using StandardScaler to ensure all features are on the same scale.\n",
    "\n",
    "**Features excluded from scaling:**\n",
    "- **Date features** (Release_Year, Release_Month, Release_Day): Have meaningful absolute values\n",
    "- **Binary features** (Genres, Tags, Categories, Supported languages, Full audio languages): Already 0/1, scaling would destroy interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numeric columns\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Identify binary features (genres, tags, categories, languages) - these should NOT be scaled\n",
    "binary_features = [col for col in numeric_features if col.startswith(('Genres_', 'Tags_', 'Categories_', 'Supported languages_', 'Full audio languages_'))]\n",
    "\n",
    "# Exclude date features and binary features from scaling\n",
    "date_features = ['Release_Year', 'Release_Month', 'Release_Day']\n",
    "features_to_scale = [col for col in numeric_features \n",
    "                     if col not in date_features \n",
    "                     and col not in binary_features]\n",
    "\n",
    "print(f\"Total numeric features: {len(numeric_features)}\")\n",
    "print(f\"Date features (not scaled): {len([col for col in date_features if col in numeric_features])}\")\n",
    "print(f\"Binary features (not scaled): {len(binary_features)}\")\n",
    "print(f\"Features to scale: {len(features_to_scale)}\")\n",
    "print(f\"\\nContinuous features being scaled: {features_to_scale}\")\n",
    "\n",
    "# Apply StandardScaler only to continuous numeric features\n",
    "scaler = StandardScaler()\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Scaling Complete\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSample of scaled continuous features:\")\n",
    "if len(features_to_scale) >= 3:\n",
    "    print(df[features_to_scale[:3]].head())\n",
    "else:\n",
    "    print(df[features_to_scale].head())\n",
    "    \n",
    "print(\"\\nSample of unscaled binary features (should be 0 or 1):\")\n",
    "if len(binary_features) >= 3:\n",
    "    print(df[binary_features[:3]].head())\n",
    "else:\n",
    "    print(df[binary_features].head())\n",
    "    \n",
    "print(\"\\nSample of unscaled date features:\")\n",
    "print(df[[col for col in date_features if col in df.columns]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93f7b2",
   "metadata": {},
   "source": [
    "# Final Dataset Overview\n",
    "Review the preprocessed dataset before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL PREPROCESSED DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Total samples: {df.shape[0]}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Feature Types:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Numeric features: {len(df.select_dtypes(include=['int64', 'float64']).columns)}\")\n",
    "print(f\"Categorical features: {len(df.select_dtypes(include=['object']).columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Missing Values:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(\"-\"*60)\n",
    "print(df['popularity_class'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['popularity_class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Sample of preprocessed data:\")\n",
    "print(\"-\"*60)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b53e70",
   "metadata": {},
   "source": [
    "# Reorder Columns\n",
    "Move the target variable to the end of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75085f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move target variable to the last column\n",
    "if 'popularity_class' in df.columns:\n",
    "    cols = [col for col in df.columns if col != 'popularity_class']\n",
    "    cols.append('popularity_class')\n",
    "    df = df[cols]\n",
    "    print(\"Target variable 'popularity_class' moved to the last column\")\n",
    "    print(f\"\\nColumn order (last 5 columns): {df.columns[-5:].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b815aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d4110",
   "metadata": {},
   "source": [
    "# Saving Preprocessed Dataset\n",
    "Save the preprocessed dataset to CSV for use in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f26749",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'data/processed/games_preprocessed.csv'\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to: {output_path}\")\n",
    "print(f\"File size: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76075696",
   "metadata": {},
   "source": [
    "# Summary of Preprocessing Steps\n",
    "\n",
    "This notebook completed the following preprocessing steps:\n",
    "\n",
    "1. **Data Loading**: Loaded the fixed games dataset (games_fixed.csv)\n",
    "\n",
    "2. **Feature Selection**: \n",
    "   - Removed irrelevant features (URLs, images, identifiers)\n",
    "   - Removed \"cheat\" features (post-launch metrics like Peak CCU, reviews, playtime)\n",
    "   - Kept \"About the game\" text field untouched for future NLP processing\n",
    "   - Parsed \"Release date\" into numeric features: Release_Year, Release_Month, Release_Day\n",
    "\n",
    "3. **Target Variable Transformation**:\n",
    "   - Removed rows with \"0 - 0\" estimated owners (noisy data)\n",
    "   - Created categorical target variable `popularity_class` with 3 levels: Low, Medium, High\n",
    "   - Removed original `Estimated owners` column\n",
    "\n",
    "4. **Data Cleaning**:\n",
    "   - Handled missing values using median imputation for numeric features\n",
    "   - Handled missing values using mode imputation for categorical features\n",
    "   - Dropped rows with remaining missing values (e.g., missing target)\n",
    "   - Removed duplicate records\n",
    "\n",
    "5. **Outlier Detection and Handling**:\n",
    "   - Detected outliers using IQR method\n",
    "   - Capped outliers to preserve data (Winsorization)\n",
    "\n",
    "6. **Feature Engineering**:\n",
    "   - Explicitly identified multi-value categorical features (Genres, Tags, Categories, Supported languages, Full audio languages)\n",
    "   - Created binary features for top 15 values in each multi-value feature\n",
    "   - Dropped original multi-value columns after encoding\n",
    "\n",
    "7. **Feature Scaling**:\n",
    "   - Applied StandardScaler only to continuous numeric features (price, DLC count, etc.)\n",
    "   - **Excluded from scaling:**\n",
    "     - Date features (Release_Year, Release_Month, Release_Day) - preserve temporal meaning\n",
    "     - Binary features (Genres_*, Tags_*, Categories_*, Supported languages_*, Full audio languages_*) - keep as 0/1 for interpretability\n",
    "\n",
    "8. **Output**:\n",
    "   - Saved preprocessed dataset to `data/processed/games_preprocessed.csv`\n",
    "   - \"About the game\" text field remains untouched and ready for NLP analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
