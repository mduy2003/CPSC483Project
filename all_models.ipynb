{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18ce1a6",
   "metadata": {},
   "source": [
    "# 4 or more ML Classifiers - Submission\n",
    "\n",
    "Will basically copy over first_model notebook things, and then make new models sections. Thinking of doing Logistic Regression, XGBoost, and SVM. Open to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7f875",
   "metadata": {},
   "source": [
    "<h3>Setup and Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff794371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n",
      "âœ“ Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"âœ“ Random seed set to: 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c3832",
   "metadata": {},
   "source": [
    "<h1>Load Preprocessed Data</h1>\n",
    "<b>Load data and perform embedded vector conversion from string to numerical</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b450f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (89302, 109)\n",
      "Total samples: 89,302\n",
      "Total features (including target): 109\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>DLC count</th>\n",
       "      <th>About the game</th>\n",
       "      <th>Windows</th>\n",
       "      <th>Mac</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Achievements</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>Release_Month</th>\n",
       "      <th>Release_Day</th>\n",
       "      <th>...</th>\n",
       "      <th>Publishers_LTD</th>\n",
       "      <th>Publishers_EroticGamesClub</th>\n",
       "      <th>Publishers_Square_Enix</th>\n",
       "      <th>Publishers_Strategy_First</th>\n",
       "      <th>Publishers_HH_Games</th>\n",
       "      <th>Publishers_Choice_of_Games</th>\n",
       "      <th>Publishers_Sekai_Project</th>\n",
       "      <th>Publishers_Electronic_Arts</th>\n",
       "      <th>Publishers_Atomic_Fabrik</th>\n",
       "      <th>popularity_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.007898</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[ 2.43655536e-02 -4.33482192e-02 -1.89679326e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.057969</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.963858</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[-1.18375435e-01  6.85120896e-02 -8.45908746e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.049630</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.338225</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[-7.47546032e-02 -1.29440166e-02  2.83202082e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.121362</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.181817</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[ 2.98691001e-02  1.47273587e-02  5.98186301e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.121362</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.118702</td>\n",
       "      <td>-0.039159</td>\n",
       "      <td>[-6.26481697e-02  7.47049646e-03 -5.16111143e-...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price  DLC count                                     About the game  \\\n",
       "0  2.007898  -0.039159  [ 2.43655536e-02 -4.33482192e-02 -1.89679326e-...   \n",
       "1 -0.963858  -0.039159  [-1.18375435e-01  6.85120896e-02 -8.45908746e-...   \n",
       "2 -0.338225  -0.039159  [-7.47546032e-02 -1.29440166e-02  2.83202082e-...   \n",
       "3 -0.181817  -0.039159  [ 2.98691001e-02  1.47273587e-02  5.98186301e-...   \n",
       "4 -1.118702  -0.039159  [-6.26481697e-02  7.47049646e-03 -5.16111143e-...   \n",
       "\n",
       "   Windows    Mac  Linux  Achievements  Release_Year  Release_Month  \\\n",
       "0     True  False  False      0.057969          2008             10   \n",
       "1     True   True  False     -0.049630          2017             10   \n",
       "2     True  False  False     -0.121362          2021             11   \n",
       "3     True   True   True     -0.121362          2020              7   \n",
       "4     True   True  False     -0.019741          2020              2   \n",
       "\n",
       "   Release_Day  ...  Publishers_LTD  Publishers_EroticGamesClub  \\\n",
       "0           21  ...               0                           0   \n",
       "1           12  ...               0                           0   \n",
       "2           17  ...               0                           0   \n",
       "3           23  ...               0                           0   \n",
       "4            3  ...               0                           0   \n",
       "\n",
       "   Publishers_Square_Enix  Publishers_Strategy_First  Publishers_HH_Games  \\\n",
       "0                       0                          0                    0   \n",
       "1                       0                          0                    0   \n",
       "2                       0                          0                    0   \n",
       "3                       0                          0                    0   \n",
       "4                       0                          0                    0   \n",
       "\n",
       "   Publishers_Choice_of_Games  Publishers_Sekai_Project  \\\n",
       "0                           0                         0   \n",
       "1                           0                         0   \n",
       "2                           0                         0   \n",
       "3                           0                         0   \n",
       "4                           0                         0   \n",
       "\n",
       "   Publishers_Electronic_Arts  Publishers_Atomic_Fabrik  popularity_class  \n",
       "0                           0                         0               Low  \n",
       "1                           0                         0               Low  \n",
       "2                           0                         0               Low  \n",
       "3                           0                         0               Low  \n",
       "4                           0                         0               Low  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset with NLP embeddings\n",
    "df = pd.read_csv('data/processed/games_preprocessed.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total samples: {df.shape[0]:,}\")\n",
    "print(f\"Total features (including target): {df.shape[1]}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19759fc",
   "metadata": {},
   "source": [
    "<b>Conversion from string embedded vectors to numeric embedded vectors</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb2415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 'About the game' embeddings from strings to numeric arrays...\n",
      "âœ“ Conversion complete!\n",
      "âœ“ Sample embedding shape: (384,)\n",
      "âœ“ Sample embedding (first 10 values): [ 0.02436555 -0.04334822 -0.00189679 -0.03764986 -0.08963642  0.02961544\n",
      " -0.0579943   0.0187653   0.01877719  0.06303879]\n",
      "âœ“ Conversion complete!\n",
      "âœ“ Sample embedding shape: (384,)\n",
      "âœ“ Sample embedding (first 10 values): [ 0.02436555 -0.04334822 -0.00189679 -0.03764986 -0.08963642  0.02961544\n",
      " -0.0579943   0.0187653   0.01877719  0.06303879]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Check if 'About the game' column exists and contains string representations of arrays\n",
    "if 'About the game' in df.columns:\n",
    "    print(\"Converting 'About the game' embeddings from strings to numeric arrays...\")\n",
    "    \n",
    "    # Convert string representations to actual arrays\n",
    "    def string_to_array(s):\n",
    "        if isinstance(s, str):\n",
    "            # Remove extra whitespace and convert to numpy array\n",
    "            return np.fromstring(s.strip('[]'), sep=' ')\n",
    "        return s\n",
    "    \n",
    "    df['About the game'] = df['About the game'].apply(string_to_array)\n",
    "    \n",
    "    print(f\"âœ“ Conversion complete!\")\n",
    "    print(f\"âœ“ Sample embedding shape: {df['About the game'].iloc[0].shape}\")\n",
    "    print(f\"âœ“ Sample embedding (first 10 values): {df['About the game'].iloc[0][:10]}\")\n",
    "else:\n",
    "    print(\"'About the game' column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f867f",
   "metadata": {},
   "source": [
    "<b>Place numeric embedded vectors back into the 'About the game' column</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716ddf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding embeddings into separate columns...\n",
      "âœ“ Expanded embeddings into 384 numeric columns\n",
      "âœ“ New dataset shape: (89302, 492)\n",
      "âœ“ First few column names: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4']\n",
      "âœ“ Last few column names: ['Publishers_Choice_of_Games', 'Publishers_Sekai_Project', 'Publishers_Electronic_Arts', 'Publishers_Atomic_Fabrik', 'popularity_class']\n",
      "âœ“ Expanded embeddings into 384 numeric columns\n",
      "âœ“ New dataset shape: (89302, 492)\n",
      "âœ“ First few column names: ['embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4']\n",
      "âœ“ Last few column names: ['Publishers_Choice_of_Games', 'Publishers_Sekai_Project', 'Publishers_Electronic_Arts', 'Publishers_Atomic_Fabrik', 'popularity_class']\n"
     ]
    }
   ],
   "source": [
    "# Expand embeddings into separate columns\n",
    "# This is necessary because sklearn models need 2D numeric arrays, not arrays within cells\n",
    "\n",
    "if 'About the game' in df.columns:\n",
    "    print(\"Expanding embeddings into separate columns...\")\n",
    "    \n",
    "    # Convert the 'About the game' column (which contains arrays) into separate columns\n",
    "    embeddings_list = df['About the game'].tolist()\n",
    "    embeddings_df = pd.DataFrame(embeddings_list, \n",
    "                                  columns=[f'embedding_{i}' for i in range(len(embeddings_list[0]))])\n",
    "    \n",
    "    # Drop the original 'About the game' column\n",
    "    df = df.drop('About the game', axis=1)\n",
    "    \n",
    "    # Insert embedding columns at the beginning\n",
    "    df = pd.concat([embeddings_df, df], axis=1)\n",
    "    \n",
    "    print(f\"âœ“ Expanded embeddings into {len(embeddings_list[0])} numeric columns\")\n",
    "    print(f\"âœ“ New dataset shape: {df.shape}\")\n",
    "    print(f\"âœ“ First few column names: {df.columns[:5].tolist()}\")\n",
    "    print(f\"âœ“ Last few column names: {df.columns[-5:].tolist()}\")\n",
    "else:\n",
    "    print(\"No embedding expansion needed - 'About the game' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a99e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution:\n",
      "popularity_class\n",
      "Low       78429\n",
      "Medium     8934\n",
      "High       1939\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution:\n",
      "popularity_class\n",
      "Low       87.824461\n",
      "Medium    10.004255\n",
      "High       2.171284\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target variable distribution:\")\n",
    "print(df['popularity_class'].value_counts())\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(df['popularity_class'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb33d2",
   "metadata": {},
   "source": [
    "<h1>Prepared Raw and Processed Data for Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1449e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (89302, 491)\n",
      "Target (y) shape: (89302,)\n",
      "\n",
      "Total number of features: 491\n",
      "\n",
      "Feature data types:\n",
      "float64    387\n",
      "int64      101\n",
      "bool         3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('popularity_class', axis=1)\n",
    "y = df['popularity_class']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nTotal number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature data types:\")\n",
    "print(X.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9968a6",
   "metadata": {},
   "source": [
    "<b>Encode Target Variable for XGBoost Compatibility</b>\n",
    "\n",
    "XGBoost requires the target variable to be numeric. This section converts string labels ('High', 'Low', 'Medium') to numeric values (0, 1, 2) using LabelEncoder. All models will use these encoded labels for consistency and efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144a2655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable encoded to numeric values\n",
      "Class mapping: {'High': np.int64(0), 'Low': np.int64(1), 'Medium': np.int64(2)}\n",
      "Target shape: (89302,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable (convert 'High', 'Low', 'Medium' to numeric)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Target variable encoded to numeric values\")\n",
    "print(f\"Class mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91ebc8",
   "metadata": {},
   "source": [
    "<b>Ensure all data is in some numeric form</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cc152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for non-numeric columns...\n",
      "\n",
      "Data types in features:\n",
      "float64    387\n",
      "int64      101\n",
      "bool         3\n",
      "Name: count, dtype: int64\n",
      "âœ“ All columns are numeric!\n",
      "\n",
      "Final feature set shape: (89302, 491)\n",
      "Final feature types:\n",
      "float64    387\n",
      "int64      101\n",
      "bool         3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns in X\n",
    "print(\"Checking for non-numeric columns...\")\n",
    "print(f\"\\nData types in features:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "# Identify object (string) columns\n",
    "object_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if object_cols:\n",
    "    print(f\"\\nâš  Found {len(object_cols)} non-numeric columns:\")\n",
    "    for col in object_cols:\n",
    "        print(f\"  - {col}: {X[col].nunique()} unique values\")\n",
    "        print(f\"    Sample values: {X[col].dropna().head(3).tolist()}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Dropping non-numeric columns: {object_cols}\")\n",
    "    X = X.drop(columns=object_cols)\n",
    "    print(f\"âœ“ Remaining features: {X.shape[1]}\")\n",
    "else:\n",
    "    print(\"âœ“ All columns are numeric!\")\n",
    "\n",
    "print(f\"\\nFinal feature set shape: {X.shape}\")\n",
    "print(f\"Final feature types:\\n{X.dtypes.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93235bd4",
   "metadata": {},
   "source": [
    "<b>Split data into training and test sets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956d6513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 71441 samples\n",
      "Testing set size: 17861 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets (80/20 split)\n",
    "# Use stratify to maintain class distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f9bd3",
   "metadata": {},
   "source": [
    "<h1>Re-perform Split Again after using SelectKBest to Find 20 Best Features</h1>\n",
    "<b>Perform SelectKBest on the data with ANOVA F-statistic</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1adee439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before SelectKBest with ANOVA:  (89302, 491)\n",
      "Features after SelectKBest with ANOVA:   (89302, 20)\n",
      "Features after SelectKBest with ANOVA:   (89302, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Features before SelectKBest with ANOVA: \", X.shape)\n",
    "X_new = SelectKBest(f_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "print(\"Features after SelectKBest with ANOVA:  \", X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39df02",
   "metadata": {},
   "source": [
    "<b>Recreate test split in X_newTrain, X_NewTest, Y_NewTrain, Y_NewTest</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f2d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 71441 samples\n",
      "Testing set size: 17861 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets (80/20 split)\n",
    "# Use stratify to maintain class distribution in both sets\n",
    "X_NewTrain, X_NewTest, y_NewTrain, y_NewTest = train_test_split(\n",
    "    X_new, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41692199",
   "metadata": {},
   "source": [
    "<h1>Model 1-1: Random Forest with Full Feature Set</h1>\n",
    "  \n",
    "<b>Define and Fit the Model for Learning</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca4900cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1 with full feature set...\n",
      "âœ“ Model 1 training complete!\n",
      "âœ“ Model 1 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier for Model 1 (Full Feature Set)\n",
    "model_M1 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Model 1 with full feature set...\")\n",
    "# Fit the model on training set\n",
    "model_M1.fit(X_train, y_train)\n",
    "print(\"âœ“ Model 1 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9344c42",
   "metadata": {},
   "source": [
    "<b>Save Fitted Model to Disk and Evaluate Model's Performance with accuracy and F1-Score</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863235ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model 1 saved as: finalized_model_M1-1.sav\n",
      "âœ“ Model 1 loaded and verified\n",
      "âœ“ Test score: 0.8839370695929679\n",
      "âœ“ Model 1 loaded and verified\n",
      "âœ“ Test score: 0.8839370695929679\n",
      "âœ“ F1score: 0.8377274845027388\n",
      "âœ“ F1score: 0.8377274845027388\n"
     ]
    }
   ],
   "source": [
    "# Save Model 1 to disk\n",
    "import sklearn\n",
    "filename_M1 = 'finalized_model_M1-1.sav'\n",
    "pickle.dump(model_M1, open(filename_M1, 'wb'))\n",
    "print(f\"âœ“ Model 1 saved as: {filename_M1}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_M1 = pickle.load(open(filename_M1, 'rb'))\n",
    "result_M1 = loaded_model_M1.score(X_test, y_test)\n",
    "print(f\"âœ“ Model 1 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_M1}\")\n",
    "\n",
    "# Use f1 score as well since accuracy may be insufficient for imbalanced classes\n",
    "# f1 = 2 * [(precision*recall)/(precision+recall)]\n",
    "# this way we can account for precision and recall as well as accuracy\n",
    "y_pred = loaded_model_M1.predict(X_test)\n",
    "modelF1Score = sklearn.metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"âœ“ F1score: {modelF1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09bee8",
   "metadata": {},
   "source": [
    "<h1>Model 1-2: Random Forest with Reduced (SelectKBest 20) Features</h1>\n",
    "  \n",
    "<b>Load and Fit Model with New Reduced Feature Set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7664ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 2 with full feature set...\n",
      "âœ“ Model 2 training complete!\n",
      "âœ“ Model 2 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest Classifier for Model 2 (Best Features)\n",
    "model_M2 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Model 2 with full feature set...\")\n",
    "# Fit the model on training set\n",
    "model_M2.fit(X_NewTrain, y_NewTrain)\n",
    "print(\"âœ“ Model 2 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f827a3",
   "metadata": {},
   "source": [
    "<b>Save Fitted Model to Disk and Evaluate Model's Performance with accuracy and F1-Score</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3571cfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model 2 saved as: finalized_model_M1-2.sav\n",
      "âœ“ Model 2 loaded and verified\n",
      "âœ“ Test score: 0.8877442472425956\n",
      "âœ“ F1score: 0.8727289513267698\n",
      "âœ“ F1score: 0.8727289513267698\n"
     ]
    }
   ],
   "source": [
    "# Save Model 2 to disk\n",
    "import sklearn\n",
    "filename_M2 = 'finalized_model_M1-2.sav'\n",
    "pickle.dump(model_M2, open(filename_M2, 'wb'))\n",
    "print(f\"âœ“ Model 2 saved as: {filename_M2}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_M2 = pickle.load(open(filename_M2, 'rb'))\n",
    "result_M2 = loaded_model_M2.score(X_NewTest, y_NewTest)\n",
    "print(f\"âœ“ Model 2 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_M2}\")\n",
    "\n",
    "# Use f1 score as well since accuracy may be insufficient for imbalanced classes\n",
    "# f1 = 2 * [(precision*recall)/(precision+recall)]\n",
    "# this way we can account for precision and recall as well as accuracy\n",
    "y_NewPred = loaded_model_M2.predict(X_NewTest)\n",
    "model2F1Score = sklearn.metrics.f1_score(y_NewTest, y_NewPred, average='weighted')\n",
    "print(f\"âœ“ F1score: {model2F1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf47ddd",
   "metadata": {},
   "source": [
    "# Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25f10d",
   "metadata": {},
   "source": [
    "## Model 2-1: Logistic Regression with Full Feature Set\n",
    "\n",
    "Logistic Regression is a linear model that's simple, fast, and interpretable. It works well as a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e025c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression Model 2-1 with full feature set...\n",
      "âœ“ Logistic Regression Model 2-1 training complete!\n",
      "âœ“ Logistic Regression Model 2-1 training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize Logistic Regression classifier\n",
    "model_LR1 = LogisticRegression(\n",
    "    max_iter=1000,  # Increase iterations for convergence with many features\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression Model 2-1 with full feature set...\")\n",
    "# Fit the model on training set\n",
    "model_LR1.fit(X_train, y_train)\n",
    "print(\"âœ“ Logistic Regression Model 2-1 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb13a0f",
   "metadata": {},
   "source": [
    "**Save Model and Evaluate Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e0545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Logistic Regression Model 2-1 saved as: finalized_model_M2-1.sav\n",
      "âœ“ Model 2-1 loaded and verified\n",
      "âœ“ Test score: 0.8902077151335311\n",
      "âœ“ F1score: 0.8676379194316232\n"
     ]
    }
   ],
   "source": [
    "# Save Logistic Regression Model 2-1 to disk\n",
    "filename_LR1 = 'finalized_model_M2-1.sav'\n",
    "pickle.dump(model_LR1, open(filename_LR1, 'wb'))\n",
    "print(f\"âœ“ Logistic Regression Model 2-1 saved as: {filename_LR1}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_LR1 = pickle.load(open(filename_LR1, 'rb'))\n",
    "result_LR1 = loaded_model_LR1.score(X_test, y_test)\n",
    "print(f\"âœ“ Model 2-1 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_LR1}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "y_pred_LR1 = loaded_model_LR1.predict(X_test)\n",
    "modelLR1_F1Score = f1_score(y_test, y_pred_LR1, average='weighted')\n",
    "print(f\"âœ“ F1score: {modelLR1_F1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973c864",
   "metadata": {},
   "source": [
    "## Model 2-2: Logistic Regression with Reduced Feature Set\n",
    "\n",
    "Train Logistic Regression using the 20 best features selected by SelectKBest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc72cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression Model 2-2 with reduced feature set (20 features)...\n",
      "âœ“ Logistic Regression Model 2-2 training complete!\n",
      "âœ“ Logistic Regression Model 2-2 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression classifier for reduced features\n",
    "model_LR2 = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression Model 2-2 with reduced feature set (20 features)...\")\n",
    "# Fit the model on training set with selected features\n",
    "model_LR2.fit(X_NewTrain, y_NewTrain)\n",
    "print(\"âœ“ Logistic Regression Model 2-2 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9682b5",
   "metadata": {},
   "source": [
    "**Save Model and Evaluate Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b793b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model saved as finalized_model_M2-2.sav\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "Logistic Regression Model 2-2 Performance:\n",
      "  Accuracy: 0.8838 (88.38%)\n",
      "  F1 Score (weighted): 0.8548 (85.48%)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "filename = 'finalized_model_M2-2.sav'\n",
    "pickle.dump(model_LR2, open(filename, 'wb'))\n",
    "print(f\"âœ“ Model saved as {filename}\")\n",
    "\n",
    "# Load the model to verify it was saved correctly\n",
    "loaded_model_LR2 = pickle.load(open(filename, 'rb'))\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_LR2 = loaded_model_LR2.predict(X_NewTest)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_LR2 = accuracy_score(y_NewTest, y_pred_LR2)\n",
    "f1_LR2 = f1_score(y_NewTest, y_pred_LR2, average='weighted')\n",
    "\n",
    "print(f\"\\nLogistic Regression Model 2-2 Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_LR2:.4f} ({accuracy_LR2*100:.2f}%)\")\n",
    "print(f\"  F1 Score (weighted): {f1_LR2:.4f} ({f1_LR2*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd12ff",
   "metadata": {},
   "source": [
    "# Model 3: XGBoost (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369140a",
   "metadata": {},
   "source": [
    "## Model 3-1: XGBoost with Full Feature Set\n",
    "\n",
    "XGBoost is a powerful ensemble method that uses gradient boosting. It's known for high performance on structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d71a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost Model 3-1 with full feature set...\n",
      "âœ“ XGBoost Model 3-1 training complete!\n",
      "âœ“ XGBoost Model 3-1 training complete!\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "model_XGB1 = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'  # For multi-class classification\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost Model 3-1 with full feature set...\")\n",
    "# Fit the model on training set\n",
    "model_XGB1.fit(X_train, y_train)\n",
    "print(\"âœ“ XGBoost Model 3-1 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9287e3",
   "metadata": {},
   "source": [
    "**Save Model and Evaluate Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a932d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ XGBoost Model 3-1 saved as: finalized_model_M3-1.sav\n",
      "âœ“ Model 3-1 loaded and verified\n",
      "âœ“ Test score: 0.9054924136386541\n",
      "âœ“ F1score: 0.8915198984451778\n"
     ]
    }
   ],
   "source": [
    "# Save XGBoost Model 3-1 to disk\n",
    "filename_XGB1 = 'finalized_model_M3-1.sav'\n",
    "pickle.dump(model_XGB1, open(filename_XGB1, 'wb'))\n",
    "print(f\"âœ“ XGBoost Model 3-1 saved as: {filename_XGB1}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_XGB1 = pickle.load(open(filename_XGB1, 'rb'))\n",
    "result_XGB1 = loaded_model_XGB1.score(X_test, y_test)\n",
    "print(f\"âœ“ Model 3-1 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_XGB1}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "y_pred_XGB1 = loaded_model_XGB1.predict(X_test)\n",
    "modelXGB1_F1Score = f1_score(y_test, y_pred_XGB1, average='weighted')\n",
    "print(f\"âœ“ F1score: {modelXGB1_F1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c7b17",
   "metadata": {},
   "source": [
    "## Model 3-2: XGBoost with Reduced Feature Set\n",
    "\n",
    "Train XGBoost using the 20 best features selected by SelectKBest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d6c731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost Model 3-2 with reduced feature set (20 features)...\n",
      "âœ“ XGBoost Model 3-2 training complete!\n",
      "âœ“ XGBoost Model 3-2 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBoost classifier for reduced features\n",
    "model_XGB2 = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost Model 3-2 with reduced feature set (20 features)...\")\n",
    "# Fit the model on training set with selected features\n",
    "model_XGB2.fit(X_NewTrain, y_NewTrain)\n",
    "print(\"âœ“ XGBoost Model 3-2 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e94c7",
   "metadata": {},
   "source": [
    "**Save Model and Evaluate Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "773e76c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model saved as finalized_model_M3-2.sav\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "XGBoost Model 3-2 Performance:\n",
      "  Accuracy: 0.8996 (89.96%)\n",
      "  F1 Score (weighted): 0.8838 (88.38%)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "filename_XGB2 = 'finalized_model_M3-2.sav'\n",
    "pickle.dump(model_XGB2, open(filename_XGB2, 'wb'))\n",
    "print(f\"âœ“ Model saved as {filename_XGB2}\")\n",
    "\n",
    "# Load the model to verify it was saved correctly\n",
    "loaded_model_XGB2 = pickle.load(open(filename_XGB2, 'rb'))\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_XGB2 = loaded_model_XGB2.predict(X_NewTest)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_XGB2 = accuracy_score(y_NewTest, y_pred_XGB2)\n",
    "f1_XGB2 = f1_score(y_NewTest, y_pred_XGB2, average='weighted')\n",
    "\n",
    "print(f\"\\nXGBoost Model 3-2 Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_XGB2:.4f} ({accuracy_XGB2*100:.2f}%)\")\n",
    "print(f\"  F1 Score (weighted): {f1_XGB2:.4f} ({f1_XGB2*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014fb62",
   "metadata": {},
   "source": [
    "# Model 4: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c83576",
   "metadata": {},
   "source": [
    "## Model 4-1: SVM with Full Feature Set\n",
    "\n",
    "SVM is effective for high-dimensional spaces. Using RBF kernel for non-linear classification.\n",
    "\n",
    "**Note:** SVM can be slow with large datasets and many features, so this may take some time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd058209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM Model 4-1 with full feature set...\n",
      "âš  This may take several minutes due to the large feature set...\n",
      "âœ“ SVM Model 4-1 training complete!\n",
      "âœ“ SVM Model 4-1 training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize SVM classifier\n",
    "model_SVM1 = SVC(\n",
    "    kernel='rbf',  # Radial basis function kernel\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training SVM Model 4-1 with full feature set...\")\n",
    "print(\"âš  This may take several minutes due to the large feature set...\")\n",
    "# Fit the model on training set\n",
    "model_SVM1.fit(X_train, y_train)\n",
    "print(\"âœ“ SVM Model 4-1 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe157",
   "metadata": {},
   "source": [
    "**Save Model and Evaluate Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b088e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SVM Model 4-1 saved as: finalized_model_M4-1.sav\n",
      "âœ“ Model 4-1 loaded and verified\n",
      "âœ“ Test score: 0.8782263031185265\n",
      "âœ“ Model 4-1 loaded and verified\n",
      "âœ“ Test score: 0.8782263031185265\n",
      "âœ“ F1score: 0.8212870176598328\n",
      "âœ“ F1score: 0.8212870176598328\n"
     ]
    }
   ],
   "source": [
    "# Save SVM Model 4-1 to disk\n",
    "filename_SVM1 = 'finalized_model_M4-1.sav'\n",
    "pickle.dump(model_SVM1, open(filename_SVM1, 'wb'))\n",
    "print(f\"âœ“ SVM Model 4-1 saved as: {filename_SVM1}\")\n",
    "\n",
    "# Load the model from disk to verify\n",
    "loaded_model_SVM1 = pickle.load(open(filename_SVM1, 'rb'))\n",
    "result_SVM1 = loaded_model_SVM1.score(X_test, y_test)\n",
    "print(f\"âœ“ Model 4-1 loaded and verified\")\n",
    "print(f\"âœ“ Test score: {result_SVM1}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "y_pred_SVM1 = loaded_model_SVM1.predict(X_test)\n",
    "modelSVM1_F1Score = f1_score(y_test, y_pred_SVM1, average='weighted')\n",
    "print(f\"âœ“ F1score: {modelSVM1_F1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a71f28",
   "metadata": {},
   "source": [
    "## Model 4-2: SVM with Reduced Feature Set\n",
    "\n",
    "Train SVM using the 20 best features. This should train much faster than the full feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dcce865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM Model 4-2 with reduced feature set (20 features)...\n",
      "âœ“ SVM Model 4-2 training complete!\n",
      "âœ“ SVM Model 4-2 training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize SVM classifier for reduced features\n",
    "model_SVM2 = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training SVM Model 4-2 with reduced feature set (20 features)...\")\n",
    "# Fit the model on training set with selected features\n",
    "model_SVM2.fit(X_NewTrain, y_NewTrain)\n",
    "print(\"âœ“ SVM Model 4-2 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbaf63",
   "metadata": {},
   "source": [
    "**Save Model and Evaluate Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2478e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model saved as finalized_model_M4-2.sav\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "SVM Model 4-2 Performance:\n",
      "  Accuracy: 0.8782 (87.82%)\n",
      "  F1 Score (weighted): 0.8213 (82.13%)\n",
      "\n",
      "SVM Model 4-2 Performance:\n",
      "  Accuracy: 0.8782 (87.82%)\n",
      "  F1 Score (weighted): 0.8213 (82.13%)\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "filename_SVM2 = 'finalized_model_M4-2.sav'\n",
    "pickle.dump(model_SVM2, open(filename_SVM2, 'wb'))\n",
    "print(f\"âœ“ Model saved as {filename_SVM2}\")\n",
    "\n",
    "# Load the model to verify it was saved correctly\n",
    "loaded_model_SVM2 = pickle.load(open(filename_SVM2, 'rb'))\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_SVM2 = loaded_model_SVM2.predict(X_NewTest)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_SVM2 = accuracy_score(y_NewTest, y_pred_SVM2)\n",
    "f1_SVM2 = f1_score(y_NewTest, y_pred_SVM2, average='weighted')\n",
    "\n",
    "print(f\"\\nSVM Model 4-2 Performance:\")\n",
    "print(f\"  Accuracy: {accuracy_SVM2:.4f} ({accuracy_SVM2*100:.2f}%)\")\n",
    "print(f\"  F1 Score (weighted): {f1_SVM2:.4f} ({f1_SVM2*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a28f5c",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook implements **4 different ML classification techniques**, each with full and reduced feature sets:\n",
    "\n",
    "1. **Random Forest** (Model 1-1, 1-2)\n",
    "2. **Logistic Regression** (Model 2-1, 2-2)\n",
    "3. **XGBoost** (Model 3-1, 3-2)\n",
    "4. **Support Vector Machine** (Model 4-1, 4-2)\n",
    "\n",
    "All models are saved as `.sav` files and evaluated using accuracy and weighted F1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
